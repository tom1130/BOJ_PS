# -*- coding: utf-8 -*-
"""base.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hnie2Qu1XtOqiRQY_PtMZCkDivoZ8rxo
"""

!pip install tensorflow==2.0.0

# augmentation

"""## 1.Sequential 모델 사용
- one-hot이 아닐경우 => sparse categorical cross entropy
- one-hot => categorical cross entropy
"""

from tensorflow.keras.layers import MaxPool2D, Conv2D, Flatten, Dense, Dropout
from tensorflow.keras.models import Sequential

model1 = Sequential([
    Conv2D(10,5,input_shape=(28,28,1)),
    MaxPool2D(pool_size=(2,2)),
    Conv2D(20,5),
    MaxPool2D(pool_size=(2,2)),
    Flatten(),
    Dropout(0.5),
    Dense(100),
    Dense(10)
])
model1.summary()

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten 

tf.random.set_seed(100)
 
(train_x, train_y), (test_x, test_y) = mnist.load_data()
train_x = train_x[:10000]
test_x = test_x[:10000]
train_y = train_y[:10000]
test_y = test_y[:10000]

############################# 이 영역의 코드를 수정하세요 ##################################

train_x = train_x.reshape((train_x.shape[0], 28, 28, 1))
test_x = test_x.reshape((test_x.shape[0], 28, 28, 1))

# one hot encoding of labels
#train_y = to_categorical(train_y)
#test_y = to_categorical(test_y)

train_norm = train_x / 255.0
test_norm = test_x / 255.0

model2 = Sequential()
model2.add(Conv2D(16, (3, 3), activation='relu', kernel_initializer='glorot_normal', input_shape=(28, 28, 1)))
model2.add(MaxPooling2D((2, 2)))
model2.add(Flatten())
model2.add(Dense(15, activation='relu', kernel_initializer='glorot_normal'))
model2.add(Dense(10, activation='softmax'))

opt = tf.keras.optimizers.SGD(lr=0.1, momentum=0.9)
model2.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
###############################################################################################


model2.fit(train_norm, train_y, epochs=5, batch_size=32, verbose=1)
print('\n=== Result for the test dataset is as follows ===')
model2.evaluate(test_norm, test_y, verbose=1) 


#가능한 답:

#train_y = to_categorical(trainY)
#test_y = to_categorical(testY)
#&
#opt = SGD(lr=0.1, momentum=0.9)
#&
#model2.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
#또는
#model2.compile(optimizer=opt, loss='mean_squared_error', metrics=['accuracy'])

"""## 2. callback learningrate"""

import tensorflow as tf

''' MNIST data setting'''
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
y_train = tf.one_hot(y_train, depth=10)
y_test = tf.one_hot(y_test, depth=10) 

''' Feedforward Model setting'''
model3 = tf.keras.models.Sequential([
###### 여기에 코드를 작성하시오 ######
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(50, activation='relu'),
  tf.keras.layers.Dense(10, activation='softmax')
######################################
])

''' Model build '''

###### 여기에 코드를 작성하시오 ######
def lr_schedule(epoch):
  if epoch < 10:
    lr = 0.1 + (epoch * 0.1) 
  else:
    lr = 1.0 - (epoch * 0.04)
  return lr

callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule, verbose=1)

model3.compile(optimizer=tf.keras.optimizers.SGD(), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])  
######################################
history = model3.fit(x_train, y_train, epochs=25, validation_data=(x_test, y_test), callbacks=[callback], verbose=1)

"""## 3.inputx 를 사용하여 함수 생성"""

import tensorflow as tf
from tensorflow.keras import layers
import numpy as np

tf.random.set_seed(2020)

def unpickle(file):
    import pickle
    with open(file, 'rb') as fo:
        dict = pickle.load(fo, encoding = 'bytes')
        return dict

train1 = unpickle('./data_batch_1')
x_train = train1[b'data'].reshape(10000,32,32,3)
y_train = train1[b'labels']
y_train = np.array(y_train, dtype = 'uint8')
y_train = np.expand_dims(y_train, axis=1)
x_train = x_train / 255.0

inputs = tf.keras.Input(shape=(32, 32, 3))
x = inputs
x = layers.Conv2D(16, 5, activation='relu', padding='same')(x)
x = layers.MaxPooling2D(2,2)(x)
x = layers.Conv2D(16, 5, activation='relu', padding='same')(x)
x = layers.Flatten()(x)
x = layers.Dense(16)(x)
x = layers.Dense(10, activation='softmax')(x)


#####################################
outputs = x
model4_1 = tf.keras.Model(inputs, outputs)
model4_1.summary()  

#model4_1.compile(optimizer='adam',
#              loss='sparse_categorical_crossentropy',
#              metrics=['accuracy'])
#model4_1.fit(x_train[:5000], y_train[:5000], epochs=5)

inputs = tf.keras.Input(shape=(32, 32, 3))
x = inputs
##### 여기에 코드를 작성하세요 #####
x = inputs
x = layers.Conv2D(32, 3, activation='relu', padding="valid")(x)
x = layers.MaxPooling2D(2,2)(x)
x = layers.Conv2D(12, 3, activation='relu', padding="valid")(x)
x = layers.Flatten()(x)
x = layers.Dense(12)(x) 
x = layers.Dense(10, activation='softmax')(x)


#####################################
outputs = x
model4_2 = tf.keras.Model(inputs, outputs)
model4_2.summary()  

model4_2.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model4_2.fit(x_train[:5000], y_train[:5000], epochs=5)

import tensorflow as tf
from tensorflow.keras import layers 

def subnetwork(x):
    x_1 = layers.Conv2D(filters = 5, kernel_size = 3, padding='same')(x)
    
    x_2 = layers.Conv2D(filters = 5, kernel_size = 3, padding='same')(x)
    x_2 = layers.Conv2D(filters = 5, kernel_size = 3, padding='same')(x_2)
    
    x_3 = layers.MaxPooling2D(pool_size=(3, 3), strides=1, padding='same')(x)

    return layers.concatenate([x_1, x_2, x_3], axis=3)
##############################################

inputs = tf.keras.Input(shape=(32, 32, 3))
x = inputs

x = subnetwork(x)
x = layers.MaxPooling2D(pool_size=(2,2), strides=2)(x)
x = subnetwork(x)
x = layers.MaxPooling2D(pool_size=(2,2), strides=2)(x)
x = subnetwork(x)
x = layers.AveragePooling2D(pool_size=(8,8), strides=1)(x)
x = layers.Dense(5, activation='softmax')(x)
##############################################
outputs = x
model5 = tf.keras.Model(inputs, outputs)
model5.summary()

import tensorflow as tf

model_rnn_1 = tf.keras.Sequential([
    tf.keras.layers.Embedding(1000002, 32),
    tf.keras.layers.LSTM(50),
    tf.keras.layers.Dense(6, activation='softmax')
    #######################################
])  

model_rnn_1.summary()

import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences

''' 모델 개선하기 '''
model_rnn_2 = tf.keras.Sequential([

    tf.keras.layers.Embedding(1000002, 32),
    tf.keras.layers.LSTM(50, return_sequences=True),
    tf.keras.layers.LSTM(50),
    tf.keras.layers.Dense(6, activation='softmax')

    #######################################
])  

model_rnn_2.summary()

''' 
민원 문장 자르기 처리하기
'''
minwon_ids = [[123, 124, 346, 76, 66, 1221, 8762, 4574, 66, 1, 7, 999, 2, 4, 98685, 52, 10, 40, 124, 127, 789654, 27, 122, 9, 1991, 4, 8, 15, 16, 23, 42]] 
minwon_ids_result = pad_sequences(minwon_ids, maxlen=25, truncating='post')

print(minwon_ids_result)



"""## image generator"""

training_datagen = ImageDataGenerator(
    rescale=1. / 255,
    # 위의 옵션 값들을 보고 적절히 대입하여 줍니다.
    rotation_range = 40,
    width_shift_range = 0.2,
    height_shift_range = 0.2,
    shear_range = 0.2,
    zoom_range = 0.2,
    horizontal_flip = True,
    fill_mode = 'nearest',
    validation_split = 0.2
    )

training_generator = training_datagen.flow_from_directory(TRAINING_DIR, 
                                                          batch_size = 128,
                                                          class_mode='categorical',
                                                          target_size = (150,150),
                                                          subset='training',
                                                         )

validation_generator = training_datagen.flow_from_directory(TRAINING_DIR, 
                                                            batch_size = 128,
                                                          class_mode='categorical',
                                                          target_size = (150,150),
                                                            subset='validation', 
                                                            )

"""## transfer learning"""

transfer_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
transfer_model.trainable=False

model = Sequential([
    transfer_model,
    Flatten(),
    Dropout(0.5),
    Dense(512, activation='relu'),
    Dense(128, activation='relu'),
    Dense(2, activation='softmax'),
])